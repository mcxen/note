
## 数据基础知识

首先分析多大的内存能够表示10亿的数呢？

一个int型占4字节，10亿就是40亿字节（很明显就是4GB，１０亿就是１０<sup>９</sup>），也就是如果完全读入内存需要占用4GB（`１０＾９*４/1024/1024/1024 = ４G`）．

如果使用long类型存储10亿个数据，一个long类型为64位二进制，即8个字节（8B），1024B = 1KB ， 1024KB = 1M ，1024M = 1G ，那么10亿个long类型数据大小为：`1000000000*8/1024/1024/1024 = 7.5G`（约等于）


int 的最大数值是 21 亿左右，初始化为－２１亿，达到２１亿就是４２亿了，实际就是最大可以存储４２亿的数据值，

## 处理海量数据问题的四板斧

### 1. **分治**
    
    - 基本上处理海量数据的问题，分治思想都是能够解决的，只不过一般情况下不会是最优方案，但可以作为一个baseline，可以逐渐优化子问题来达到一个较优解。传统的归并排序就是分治思想，涉及到大量无法加载到内存的文件、排序等问题都可以用这个方法解决。
        
    - 适用场景:数据量大无法加载到内存
        
    - 技能链接:归并排序
        
### 2. 哈希(Hash)
    
    - 个人感觉Hash是最为粗暴的一种方式，但粗暴却高效，**唯一的缺点是耗内存，需要将数据全部载入内存。**
        
    - 适用场景:快速查找，需要总数据量可以放入内存
        
### 3. bit(位集或BitMap)
    
    - 位集这种思想其实简约而不简单，有很多扩展和技巧。比如多位表示一个数据(能够表示存在和数量问题)，BloomFilter(布隆过滤器就是一个典型的扩展)，在实际工作中应用场景很多，比如消息过滤等，读者需要掌握，但对于布隆过滤器使用有一些误区和不清楚的地方，读者可以看下面这篇博客避免这些性能上的误区。
        
    - 适用场景：可进行数据的快速查找，判重
    - 1、申请一个位图数组，长度为 1 亿，初始化为 0。然后遍历所有电话号码，把号码对应的位图中的位置置为 1。遍历完成后，如果 bit 为 1，则表示这个电话号码在文件中存在，否则不存在。bit 值为 1 的数量即为 不同电话号码的个数。
    - 2、申请的位图，可以是2位表示一个数字，00表示没有，01表示有一个，10表示有多个.
        
    - 技能链接:布隆过滤器使用的性能误区

位图法的话：

为了缩减空间，直接使用二进制来存储数据，0表示某个数据不存在，1表示某个数据存在，那么就需要10亿个0或者1来表示10亿个数据 ， ｌｏｎｇ来存储的话，存储所占的空间便是：1000000000/8/1024/1024 = 120M（约等于），从7.5G变成120M，不仅节省了大量的空间而且查重的话，我只需要判断该数据所对应的二进制位是否为1即可

在JAVA中没有直接用来表示二进制的数据类型，所以我决定还是直接存储long类型的方式来完成上面的设想，一个long类型为64位，其中包括一位符号位，所以实际可用的位数为63位，那么我就需1000000000/63 = 15873016个（一千六百万）long类型的数据，这里就需要 `1000000000/63*8/1024/1024 = 122M`（约等于）的空间。

那么如何才能知道15873016个long类型的数据中，每一个所代表的63位具体数据是什么呢，其实很简单，都已经分好了每63个数据放在一个long类型的二进制中，那给每一个long类型一个索引即可，索引的计算就是具体的数据/63，比如0/63 = 0 ， 1/63 = 0 ， 5/63 = 0 ， 62/63 = 0，也就是说`[0,62]`区间内的所有整数的索引都是0，`[63,125]`索引为1 …以此类推，索引存储为int类型，一共有15873016个一千六百万个索引，占据的空间就是：`15873016 * 4/1024*1024 = 61M`（约等于）

从上面来看，所需要的总空间大小大约为 122 + 61 = 183M ， 没超过500M，就是要500M 空间，那也远比7.5G要小。

HashMap<Interger,Long>来存储全部数据 ， Interger便是我上述 所谓的索引，Long便是每一个数据


如果位图还是超过了内存的话，
32位int的范围是42亿，40亿整数中肯定有一些是连续的，我们可以先对数据进行一个外部排序，然后用一个初始的数和一个长度构成一个数据结构，来表示一段连续的数，举个例子。

如果数据是1 2 3 4 6 7……这种的，那么可以用(1,4)和(6,2)来表示，这样一来，连续的数都变成了2个数表示。

来了一个新数之后，就用二分法进行查找了。

这样一来，最差情况就是2亿多的断点，也就是2亿多的结构体，每个结构体8个字节，大概16亿字节，1.6GB，在内存中可以放下。

### 4. 堆(Heap)
    
    - 堆排序是一种比较通用的TopN问题解决方案，能够满足绝大部分的求最值的问题，读者需要掌握堆的基本操作和思想。
        
    - 适用场景:处理海量数据中TopN的问题(最大或最小)，要求N不大，使得堆可以放入内存
        
    - 技能链接:排序算法-Heap排序

每个结点的值都大于或等于其左右孩子结点的值，称为大顶堆；或者每个结点的值都小于或等于其左右孩子结点的值，称为小顶堆。如下图

![img](https://raw.githubusercontent.com/52chen/imagebed2023/main/picgo/1024555-20161217182750011-675658660.png)


TOP Ｎ：

保存一个k个元素的小顶堆，前k个元素依次加入小顶堆（第k大的元素就是堆顶）。
后面的元素，首先和堆顶的元素比较，若大于堆顶的元素，则移除堆顶的元素，然后将该元素加入到堆中，这里不是说把这个元素直接放在堆顶，而是说是放在这个堆里面就可以了。那么这个堆始终保持着目前遍历的元素中第k大的元素在堆顶。

```ｊａｖａ
class Solution {  
    public int findKthLargest(int[] nums, int k) {  
        //最小堆，堆顶就是最小,比较元素和堆顶的大小，如果比他大，就更新这个堆  
        PriorityQueue<Integer> heap = new PriorityQueue<>(k);  
        for (int i = 0; i < nums.length; i++) {  
            if (i < k) heap.add(nums[i]);  
            else{  
                if (heap.peek() < nums[i]) {  
                    heap.poll();  
                    heap.add(nums[i]);  
                }  
            }  
//            System.out.println(heap.peek());  
        }  
        return heap.peek();  
    }  
}
```



## 题目分析


### 1、如何从大量的 URL 中找出相同的 URL？

**题目描述**

给定 a、b 两个文件，各存放 50 亿个 URL，每个 URL 各占 64B，内存限制是 4G。请找出 a、b 两个文件共同的 URL。

**解答思路**

每个 URL 占 64B，那么 50 亿个 URL占用的空间大小约为 320GB。
5,000,000,000 * 64B ≈ 5GB * 64 = 320GB

由于内存大小只有 4G，因此，我们不可能一次性把所有 URL 加载到内存中处理。对于这种类型的题目，一般采用分治策略，即：把一个文件中的 URL 按照某个特征划分为多个小文件，使得每个小文件大小不超过 4G，这样就可以把这个小文件读到内存中进行处理了。

**思路如下：**

首先遍历文件 a，对遍历到的 URL 求 hash(URL) % 1000，根据计算结果把遍历到的 URL 存储到 a0, a1, a2, ..., a999，这样每个大小约为 300MB。使用同样的方法遍历文件 b，把文件 b 中的 URL 分别存储到文件 b0, b1, b2, ..., b999 中。这样处理过后，所有可能相同的 URL 都在对应的小文件中，即 a0 对应 b0, ..., a999 对应 b999，不对应的小文件不可能有相同的 URL。那么接下来，我们只需要求出这 1000 对小文件中相同的 URL 就好了。
接着遍历 ai( i∈[0,999])，把 URL 存储到一个 HashSet 集合中。然后遍历 bi 中每个 URL，看在 HashSet 集合中是否存在，若存在，说明这就是共同的 URL，可以把这个 URL 保存到一个单独的文件中。

**方法总结**

分而治之，进行哈希取余；
对每个子文件进行 HashSet 统计。

### 2、如何按照 query 的频度排序？（百度）

**题目描述**
有 10 个文件，每个文件大小为 1G，每个文件的每一行存放的都是用户的 query，每个文件的 query 都可能重复。要求按照 query 的频度排序。

**解答思路**
如果 query 的重复度比较大，可以考虑一次性把所有 query 读入内存中处理；如果 query 的重复率不高，那么可用内存不足以容纳所有的 query，这时候就需要采用分治法或其他的方法来解决。

**方法一：HashMap 法**
如果 query 重复率高，说明不同 query 总数比较小，可以考虑把所有的 query 都加载到内存中的 HashMap 中。接着就可以按照 query 出现的次数进行排序。

**方法二：分治法**
分治法需要根据数据量大小以及可用内存的大小来确定问题划分的规模。对于这道题，可以顺序遍历 10 个文件中的 query，通过 Hash 函数 hash(query) % 10 把这些 query 划分到 10 个小文件中。之后对每个小文件使用 HashMap 统计 query 出现次数，根据次数排序并写入到零外一个单独文件中。
接着对所有文件按照 query 的次数进行排序，这里可以使用归并排序（由于无法把所有 query 都读入内存，因此需要使用外排序）。

**方法总结**
内存若够，直接读入进行排序；
内存不够，先划分为小文件，小文件排好序后，整理使用外排序进行归并。

### 3、如何统计不同电话号码的个数？（百度）

**题目描述**
已知某个文件内包含一些电话号码，每个号码为 8 位数字，统计不同号码的个数。

**解答思路**
这道题本质还是求解数据重复的问题，对于这类问题，一般首先考虑位图法。
对于本题，8 位电话号码可以表示的号码个数为 10^8 个，即 1 亿个。我们每个号码用一个 bit 来表示，则总共需要 1 亿个 bit，内存占用约 100M。

**思路如下：**
申请一个位图数组，长度为 1 亿，初始化为 0。然后遍历所有电话号码，把号码对应的位图中的位置置为 1。遍历完成后，如果 bit 为 1，则表示这个电话号码在文件中存在，否则不存在。bit 值为 1 的数量即为 不同电话号码的个数。

**方法总结**
求解数据重复问题，记得考虑位图法。

### 4、如何从 5 亿个数中找出中位数？（百度）

**题目描述**
从 5 亿个数中找出中位数。数据排序后，位置在最中间的数就是中位数。当样本数为奇数时，中位数为 第 (N+1)/2 个数；当样本数为偶数时，中位数为 第 N/2 个数与第 1+N/2 个数的均值。

**解答思路**
如果这道题没有内存大小限制，则可以把所有数读到内存中排序后找出中位数。但是最好的排序算法的时间复杂度都为 O(NlogN)。这里使用其他方法。

**方法一：双堆法**
维护两个堆，一个大顶堆，一个小顶堆。大顶堆中最大的数小于等于小顶堆中最小的数；保证这两个堆中的元素个数的差不超过 1。

若数据总数为偶数，当这两个堆建好之后，中位数就是这两个堆顶元素的平均值。当数据总数为奇数时，根据两个堆的大小，中位数一定在数据多的堆的堆顶。

```java
class MedianFinder {
    
    private PriorityQueue<Integer> maxHeap;
    private PriorityQueue<Integer> minHeap;

    /** initialize your data structure here. */
    public MedianFinder() {
        maxHeap = new PriorityQueue<>(Comparator.reverseOrder());
        minHeap = new PriorityQueue<>(Integer::compareTo);
    }
    
    public void addNum(int num) {
        if (maxHeap.isEmpty() || maxHeap.peek() > num) {
            maxHeap.offer(num);
        } else {
            minHeap.offer(num);
        }
        
        int size1 = maxHeap.size();
        int size2 = minHeap.size();
        if (size1 - size2 > 1) {
            minHeap.offer(maxHeap.poll());
        } else if (size2 - size1 > 1) {
            maxHeap.offer(minHeap.poll());
        }
    }
    
    public double findMedian() {
        int size1 = maxHeap.size();
        int size2 = minHeap.size();
        
        return size1 == size2 
            ? (maxHeap.peek() + minHeap.peek()) * 1.0 / 2
            : (size1 > size2 ? maxHeap.peek() : minHeap.peek());
    }
}
```

以上这种方法，需要把所有数据都加载到内存中。当数据量很大时，就不能这样了，因此，这种方法适用于数据量较小的情况。5 亿个数，每个数字占用 4B，总共需要 2G 内存。如果可用内存不足 2G，就不能使用这种方法了，下面介绍另一种方法。

**方法二：分治法**
分治法的思想是把一个大的问题逐渐转换为规模较小的问题来求解。

对于这道题，顺序读取这 5 亿个数字，对于读取到的数字 num，如果它对应的二进制中最高位为1，则把这个数字写到 f1 中，否则写入 f0 中。通过这一步，可以把这 5 亿个数划分为两部分，而且 f0 中的数都大于 f1 中的数（最高位是符号位）。

划分之后，可以非常容易地知道中位数是在 f0 还是 f1 中。假设 f1 中有 1 亿个数，那么中位数一定在 f0 中，且是在 f0 中，从小到大排列的第 1.5 亿个数与它后面的一个数的平均值。

提示，5 亿数的中位数是第 2.5 亿与右边相邻一个数求平均值。若 f1 有一亿个数，那么中位数就是 f0 中从第 1.5 亿个数开始的两个数求得的平均值。

对于 f0 可以用次高位的二进制继续将文件一分为二，如此划分下去，直到划分后的文件可以被加载到内存中，把数据加载到内存中以后直接排序，找出中位数。

注意，当数据总数为偶数，如果划分后两个文件中的数据有相同个数，那么中位数就是数据较小的文件中的最大值与数据较大的文件中的最小值的平均值。

**方法总结**
分治法，真香！

### 5、如何从大量数据中找出高频词 TOP N？（百度）

**题目描述**
有一个 1GB 大小的文件，文件里每一行是一个词，每个词的大小不超过 16B，内存大小限制是 1MB，要求返回频数最高的 100 个词(Top 100)。

**解答思路**
由于内存限制，我们依然无法直接将大文件的所有词一次读到内存中。因此，同样可以采用分治策略，把一个大文件分解成多个小文件，保证每个文件的大小小于 1MB，进而直接将单个小文件读取到内存中进行处理。

**思路如下：**
首先遍历大文件，对遍历到的每个词x，执行 hash(x) % 5000，将结果为 i 的词存放到文件 a_i 中。遍历结束后，我们可以得到 5000 个小文件。每个小文件的大小为 200KB 左右。如果有的小文件大小仍然超过 1MB，则采用同样的方式继续进行分解。

接着统计每个小文件中出现频数最高的 100 个词。最简单的方式是使用 HashMap 来实现。其中 key 为词，value 为该词出现的频率。具体方法是：对于遍历到的词 x，如果在 map 中不存在，则执行 map.put(x, 1)；若存在，则执行 map.put(x, map.get(x)+1)，将该词频数加 1。

上面我们统计了每个小文件单词出现的频数。接下来，我们可以通过维护一个小顶堆来找出所有词中出现频数最高的 100 个。具体方法是：依次遍历每个小文件，构建一个小顶堆，堆大小为 100。如果遍历到的词的出现次数大于堆顶词的出现次数，则用新词替换堆顶的词，然后重新调整为小顶堆，遍历结束后，小顶堆上的词就是出现频数最高的 100 个词。

**方法总结**
分而治之，进行哈希取余；
使用 HashMap 统计频数；
求解最大的 TopN 个，用小顶堆；这个是因为小顶堆堆首部是个最小值，元素挨个和最小值比较，如果比这个最小值大，那就把这个堆顶ｐｏｌｌ，直接插入这个元素，ａｄｄ，堆会自动把最小值选出来，放到堆顶，ｐｅｅｋ（）；

求解最小的 TopN 个，用大顶堆。

### 5.1 10亿个搜索关键词日志文件，获取Top 10 1GB内存

10亿=10^9 
可将10亿条搜索关键词先通过哈希算法分片到10个文件：

创建10个空文件：00~09

遍历这10亿个关键词，并通过某哈希算法求哈希值,
哈希值同10取模，结果就是该搜索关键词应被分到的文件编号,
10亿关键词分片后，每个文件都只有1亿关键词，去掉重复的，可能就剩1000万，每个关键词平均50个字节，总大小500M，1G内存够。

针对每个包含1亿条搜索关键词的文件：

利用散列表和堆，分别求Top 10
10个Top 10放一起，取这100个关键词中，出现次数Top 10关键词，即得10亿数据的Top 10热搜关键词


### 6、如何找出某一天访问百度网站最多的 IP？（百度）

**题目描述**
现有海量日志数据保存在一个超大文件中，该文件无法直接读入内存，要求从中提取某天访问百度次数最多的那个 IP。

**解答思路**
这道题只关心某一天访问百度最多的 IP，因此，可以首先对文件进行一次遍历，把这一天访问百度 IP 的相关信息记录到一个单独的大文件中。接下来采用的方法与上一题一样，大致就是先对 IP 进行哈希映射，接着使用 HashMap 统计重复 IP 的次数，最后计算出重复次数最多的 IP。

注：这里只需要找出出现次数最多的 IP，可以不必使用堆，直接用一个变量 max 即可。

**方法总结**
分而治之，进行哈希取余；
使用 HashMap 统计频数；
求解最大的 TopN 个，用小顶堆；求解最小的 TopN 个，用大顶堆。

### 7、如何在大量的数据中找出不重复的整数？（百度）

**题目描述**
在 2.5 亿个整数中找出不重复的整数。注意：内存不足以容纳这 2.5 亿个整数。

**解答思路**

**方法一：分治法**
与前面的题目方法类似，先将 2.5 亿个数划分到多个小文件，用 HashSet/HashMap 找出每个小文件中不重复的整数，再合并每个子结果，即为最终结果。

**方法二：位图法**
位图，就是用一个或多个 bit 来标记某个元素对应的值，而键就是该元素。采用位作为单位来存储数据，可以大大节省存储空间。

位图通过使用位数组来表示某些元素是否存在。它可以用于快速查找，判重，排序等。不是很清楚？我先举个小例子。
假设我们要对 [0,7] 中的 5 个元素 (6, 4, 2, 1, 5) 进行排序，可以采用位图法。0~7 范围总共有 8 个数，只需要 8bit，即 1 个字节。首先将每个位都置 0：

```plain
0 0 0 0 0 0 0 0
```

然后遍历 5 个元素，首先遇到 6，那么将下标为 6 的位的 0 置为 1；接着遇到 4，把下标为 4 的位 的 0 置为 1：

```plain
0 0 0 0 1 0 1 0
```

依次遍历，结束后，位数组是这样的：

```plain
0 1 1 0 1 1 1 0
```

每个为 1 的位，它的下标都表示了一个数：

```plain
for i in range(8):
    if bits[i] == 1:
        print(i)
```

这样我们其实就已经实现了排序。
对于整数相关的算法的求解，位图法是一种非常实用的算法。假设 int 整数占用 4B，即 32bit，那么我们可以表示的整数的个数为 232。
那么对于这道题，我们用 2 个 bit 来表示各个数字的状态：
00 表示这个数字没出现过；
01 表示这个数字出现过一次（即为题目所找的不重复整数）；
10 表示这个数字出现了多次。
那么这 232 个整数，总共所需内存为 2<sup>32</sup>*2 b=1GB。因此，当可用内存超过 1GB 时，可以采用位图法。假设内存满足位图法需求，进行下面的操作：
遍历 2.5 亿个整数，查看位图中对应的位，如果是 00，则变为 01，如果是 01 则变为 10，如果是 10 则保持不变。遍历结束后，**查看位图，把对应位是 01 的整数输出即可。**

**方法总结**
判断数字是否重复的问题，位图法是一种非常高效的方法。

### 8、如何在大量的数据中判断一个数是否存在？（腾讯）

**题目描述**
给定 40 亿个不重复的没排过序的 unsigned int 型整数，然后再给定一个数，如何快速判断这个数是否在这 40 亿个整数当中？

**解答思路**

**方法一：分治法**
依然可以用分治法解决，方法与前面类似，就不再次赘述了。

**方法二：位图法**
40 亿个不重复整数，我们用 40 亿个 bit 来表示，初始位均为 0，那么总共需要内存：4,000,000,000b≈512M。

我们读取这 40 亿个整数，将对应的 bit 设置为 1。接着读取要查询的数，查看相应位是否为 1，如果为 1 表示存在，如果为 0 表示不存在。

**方法总结**
判断数字是否存在、判断数字是否重复的问题，位图法是一种非常高效的方法。

### 9、如何查询最热门的查询串？（腾讯）

**题目描述**
搜索引擎会通过日志文件把用户每次检索使用的所有查询串都记录下来，每个查询床的长度不超过 255 字节。
假设目前有 1000w 个记录（这些查询串的重复度比较高，虽然总数是 1000w，但如果除去重复后，则不超过 300w 个）。请统计最热门的 10 个查询串，要求使用的内存不能超过 1G。（一个查询串的重复度越高，说明查询它的用户越多，也就越热门。）

**解答思路**
每个查询串最长为 255B，1000w 个串需要占用 约 2.55G 内存，因此，我们无法将所有字符串全部读入到内存中处理。

**方法一：分治法**
分治法依然是一个非常实用的方法。
划分为多个小文件，保证单个小文件中的字符串能被直接加载到内存中处理，然后求出每个文件中出现次数最多的 10 个字符串；最后通过一个小顶堆统计出所有文件中出现最多的 10 个字符串。
方法可行，但不是最好，下面介绍其他方法。

**方法二：HashMap 法**
虽然字符串总数比较多，但去重后不超过 300w，因此，可以考虑把所有字符串及出现次数保存在一个 HashMap 中，所占用的空间为 300w*(255+4)≈777M（其中，4表示整数占用的4个字节）。由此可见，1G 的内存空间完全够用。
思路如下：

首先，遍历字符串，若不在 map 中，直接存入 map，value 记为 1；若在 map 中，则把对应的 value 加 1，这一步时间复杂度 O(N)。

接着遍历 map，构建一个 10 个元素的小顶堆，若遍历到的字符串的出现次数大于堆顶字符串的出现次数，则进行替换，并将堆调整为小顶堆。

遍历结束后，堆中 10 个字符串就是出现次数最多的字符串。这一步时间复杂度 O(Nlog10)。

**方法三：前缀树法**
方法二使用了 HashMap 来统计次数，当这些字符串有大量相同前缀时，可以考虑使用前缀树来统计字符串出现的次数，树的结点保存字符串出现次数，0 表示没有出现。

**思路如下：**
在遍历字符串时，在前缀树中查找，如果找到，则把结点中保存的字符串次数加 1，否则为这个字符串构建新结点，构建完成后把叶子结点中字符串的出现次数置为 1。
最后依然使用小顶堆来对字符串的出现次数进行排序。

**方法总结**
前缀树经常被用来统计字符串的出现次数。它的另外一个大的用途是字符串查找，判断是否有重复的字符串等。

### 10如何找出排名前 500 的数？（腾讯）

**题目描述**
有 20 个数组，每个数组有 500 个元素，并且有序排列。如何在这 20*500 个数中找出前 500 的数？

**解答思路**
对于 TopK 问题，最常用的方法是使用堆排序。对本题而言，假设数组降序排列，可以采用以下方法：

首先建立大顶堆，堆的大小为数组的个数，即为 20，把每个数组最大的值存到堆中。
接着删除堆顶元素，保存到另一个大小为 500 的数组中，然后向大顶堆插入删除的元素所在数组的下一个元素。
重复上面的步骤，直到删除完第 500 个元素，也即找出了最大的前 500 个数。
为了在堆中取出一个数据后，能知道它是从哪个数组中取出的，从而可以从这个数组中取下一个值，可以把数组的指针存放到堆中，对这个指针提供比较大小的方法。

```java
import lombok.Data;

import java.util.Arrays;
import java.util.PriorityQueue;

@Data
public class DataWithSource implements Comparable<DataWithSource> {
    /**
     * 数值
     */
    private int value;

    /**
     * 记录数值来源的数组
     */
    private int source;

    /**
     * 记录数值在数组中的索引
     */
    private int index;

    public DataWithSource(int value, int source, int index) {
        this.value = value;
        this.source = source;
        this.index = index;
    }

    /**
     *
     * 由于 PriorityQueue 使用小顶堆来实现，这里通过修改
     * 两个整数的比较逻辑来让 PriorityQueue 变成大顶堆
     */
    @Override
    public int compareTo(DataWithSource o) {
        return Integer.compare(o.getValue(), this.value);
    }
}


class Test {
    public static int[] getTop(int[][] data) {
        int rowSize = data.length;
        int columnSize = data[0].length;

        // 创建一个columnSize大小的数组，存放结果
        int[] result = new int[columnSize];

        PriorityQueue<DataWithSource> maxHeap = new PriorityQueue<>();
        for (int i = 0; i < rowSize; ++i) {
            // 将每个数组的最大一个元素放入堆中
            DataWithSource d = new DataWithSource(data[i][0], i, 0);
            maxHeap.add(d);
        }

        int num = 0;
        while (num < columnSize) {
            // 删除堆顶元素
            DataWithSource d = maxHeap.poll();
            result[num++] = d.getValue();
            if (num >= columnSize) {
                break;
            }

            d.setValue(data[d.getSource()][d.getIndex() + 1]);
            d.setIndex(d.getIndex() + 1);
            maxHeap.add(d);
        }
        return result;

    }

    public static void main(String[] args) {
        int[][] data = {
                {29, 17, 14, 2, 1},
                {19, 17, 16, 15, 6},
                {30, 25, 20, 14, 5},
        };

        int[] top = getTop(data);
        System.out.println(Arrays.toString(top)); // [30, 29, 25, 20, 19]
    }
}
```


## 如何只用2GB内存从20亿，40亿，80亿个整数中找到出现次数最多的数？


### 20亿级别

面试官：如果我给你 2GB 的内存，并且给你 20 亿个 int 型整数，让你来找出次数出现最多的数，你会怎么做？

小秋：（嗯？怎么感觉和之前的那道判断一个数是否出现在这 40 亿个整数中有点一样？可是，如果还是采用 bitmap 算法的话，好像无法统计一个数出现的次数，只能判断一个数是否存在），我可以采用哈希表来统计，把这个数作为 key，把这个数出现的次数作为 value，之后我再遍历哈希表哪个数出现最多的次数最多就可以了。

面试官：你可以算下你这个方法需要花费多少内存吗？

小秋：key 和 value 都是 int 型整数，一个 int 型占用 4B 的内存，所以哈希表的一条记录需要占用 8B，最坏的情况下，这 20 亿个数都是不同的数，大概会占用 16GB 的内存。

面试官:你的分析是对的，然而我给你的只有 2GB 内存。

小秋：（感觉这道题有点相似，不过不知为啥，没啥思路，这下凉凉），目前没有更好的方法。

面试官：按照你那个方法的话，最多只能记录大概 2 亿多条不同的记录，2 亿多条不同的记录，大概是 1.6GB 的内存。

小秋：（嗯？面试官说这话是在提示我？）我有点思路了，我可以把这 20 亿个数存放在不同的文件，然后再来筛选。

面试题：可以具体说说吗？

小秋：刚才你说，我的那个方法，最多只能记录大概 2 亿多条的不同记录，那么我可以把这 20 亿个数映射到不同的文件中去，例如，数值在 0 至 2亿之间的存放在文件1中，数值在2亿至4亿之间的存放在文件2中....，由于 int 型整数大概有 42 亿个不同的数，所以我可以把他们映射到 21 个文件中去，如图



显然，相同的数一定会在同一个文件中，我们这个时候就可以用我的那个方法，统计每个文件中出现次数最多的数，然后再从这些数中再次选出最多的数，就可以了。

面试官：嗯，这个方法确实不错，不过，如果我给的这 20 亿个数数值比较集中的话，例如都处于 1~20000000 之间，那么你都会把他们全部映射到同一个文件中，你有优化思路吗？

小秋：那我可以先把每个数先做**哈希函数映射**，根据哈希函数得到的哈希值，再把他们存放到对应的文件中，如果哈希函数设计到好的话，那么这些数就会分布的比较平均。（关于哈希函数的设计，我就不说了，我这只是提供一种思路）

### 40亿级别

面试官：那如果我把 20 亿个数加到 40 亿个数呢？

小秋：（这还不简单，映射到42个文件呗）那我可以加大文件的数量啊。

面试官：那如果我给的这 40 亿个数中数值都是一样的，那么你的哈希表中，某个 key 的 value 存放的数值就会是 40 亿，然而 int 的最大数值是 21 亿左右，那么就会出现溢出，你该怎么办？

小秋：（那我把 int 改为 long 不就得了，虽然会占用更多的内存，那我可以把文件分多几份呗，不过，这应该不是面试官想要的答案），我可以把 value 初始值赋值为 **负21亿**，这样，如果 value 的数值是 21 亿的话，就代表某个 key 出现了 42 亿次了。

> 这里说明下，文件还是 21 个就够了，因为 21 个文件就可以把每个文件的数值种类控制在 2亿种了，也就是说，哈希表存放的记录还是不会超过 2 亿中。

### 80亿级别

面试官：反应挺快哈，那我如果把 40 亿增加到 80 亿呢？

小秋：（我靠，这变本加厉啊）.........我知道了，我可以一边遍历一遍判断啊，如果我在统计的过程中，发现某个 key 出现的次数超过了 40 亿次，那么，就不可能再有另外一个 key 出现的次数比它多了，那我直接把这个 key 返回就搞定了。




